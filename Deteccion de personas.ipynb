{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de video cargado con éxito\n",
      "Final del video.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# Cargamos el modelo preentrenado YOLO (versión pequeña para rapidez)\n",
    "model = YOLO(\"yolov8n.pt\")  # Archivo .pt contiene los pesos del modelo preentrenado\n",
    "\n",
    "# Restauramos el archivo original de clases (COCO dataset)\n",
    "classesFile = \"coco.names\"  # Asegúrate de que el archivo coco.names tenga todas las clases\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')  # Cargamos todas las clases en una lista\n",
    "    COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Iniciamos la captura de video desde un archivo\n",
    "video_path = 'video3.mp4'  # Cambia esto por la ruta de tu archivo de video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Verificamos si el video se ha cargado correctamente\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"No se pudo abrir el archivo de video.\")\n",
    "\n",
    "print(\"Archivo de video cargado con éxito\")\n",
    "\n",
    "# Función para dibujar las cajas solo para personas (clase 0) e identificarlas como Persona 1, Persona 2, etc.\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    person_id = 1  # Inicializamos el identificador para cada persona\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "\n",
    "        # Solo dibujamos cajas si la clase detectada es 'person' (clase 0 en COCO)\n",
    "        if c == 0:  # Clase 0 corresponde a 'person'\n",
    "            # Extraemos las coordenadas de la caja\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "\n",
    "            # Dibujamos la caja sobre el frame\n",
    "            color = colors[c]\n",
    "            cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 2)\n",
    "\n",
    "            # Etiquetamos a la persona como Persona 1, Persona 2, etc.\n",
    "            label = f\"Persona {person_id}\"\n",
    "            person_id += 1  # Incrementamos el contador para la siguiente persona\n",
    "\n",
    "            # Ponemos el texto con la identificación de la persona en la caja detectada\n",
    "            cv2.putText(frame, label, (r[0], r[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "# Bucle principal para capturar frames del video y procesar las detecciones\n",
    "while cap.isOpened():\n",
    "    # Leemos un frame del video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Final del video.\")\n",
    "        break\n",
    "\n",
    "    # Realizamos la predicción con el modelo YOLO\n",
    "    results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "    # Procesamos cada resultado detectado\n",
    "    for result in results:\n",
    "        draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "    # Mostramos el frame con las detecciones en una ventana de OpenCV\n",
    "    cv2.imshow('Detección de Personas en Video', frame)\n",
    "\n",
    "    # Verificamos si se presiona la tecla 'q' para salir\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos el video y cerramos todas las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cámara iniciada con éxito\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# Cargamos el modelo preentrenado YOLO (versión pequeña para rapidez)\n",
    "model = YOLO(\"yolov8n.pt\")  # Archivo .pt contiene los pesos del modelo preentrenado\n",
    "\n",
    "# Restauramos el archivo original de clases (COCO dataset)\n",
    "classesFile = \"coco.names\"  # Asegúrate de que el archivo coco.names tenga todas las clases\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')  # Cargamos todas las clases en una lista\n",
    "    COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Iniciamos la captura de video desde la cámara (índice 1 para la cámara predeterminada)\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Verificamos si la cámara se ha iniciado correctamente\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"No se pudo abrir la cámara.\")\n",
    "\n",
    "print(\"Cámara iniciada con éxito\")\n",
    "\n",
    "# Función para dibujar las cajas solo para personas (clase 0) e identificarlas como Persona 1, Persona 2, etc.\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    person_id = 1  # Inicializamos el identificador para cada persona\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "\n",
    "        # Solo dibujamos cajas si la clase detectada es 'person' (clase 0 en COCO)\n",
    "        if c == 0:  # Clase 0 corresponde a 'person'\n",
    "            # Extraemos las coordenadas de la caja\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "\n",
    "            # Dibujamos la caja sobre el frame\n",
    "            color = colors[c]\n",
    "            cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 2)\n",
    "\n",
    "            # Etiquetamos a la persona como Persona 1, Persona 2, etc.\n",
    "            label = f\"Persona {person_id}\"\n",
    "            person_id += 1  # Incrementamos el contador para la siguiente persona\n",
    "\n",
    "            # Ponemos el texto con la identificación de la persona en la caja detectada\n",
    "            cv2.putText(frame, label, (r[0], r[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "# Bucle principal para capturar frames de la cámara y procesar las detecciones\n",
    "while cap.isOpened():\n",
    "    # Leemos un frame de la cámara\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"No se pudo leer el frame de la cámara.\")\n",
    "        break\n",
    "\n",
    "    # Invertimos el frame horizontalmente\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Realizamos la predicción con el modelo YOLO\n",
    "    results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "    # Procesamos cada resultado detectado\n",
    "    for result in results:\n",
    "        draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "    # Mostramos el frame con las detecciones en una ventana de OpenCV\n",
    "    cv2.imshow('Detección de Personas - Cámara', frame)\n",
    "\n",
    "    # Verificamos si se presiona la tecla 'q' para salir\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos la cámara y cerramos todas las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Etiquetas si cuenta con la deteccion o no con colores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cámara iniciada con éxito\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# Cargamos el modelo preentrenado YOLO (versión pequeña para rapidez)\n",
    "model = YOLO(\"yolov8n.pt\")  # Archivo .pt contiene los pesos del modelo preentrenado\n",
    "\n",
    "# Restauramos el archivo original de clases (COCO dataset)\n",
    "classesFile = \"coco.names\"  # Asegúrate de que el archivo coco.names tenga todas las clases\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')  # Cargamos todas las clases en una lista\n",
    "    COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Iniciamos la captura de video desde la cámara (índice 1 para la cámara predeterminada)\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Verificamos si la cámara se ha iniciado correctamente\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"No se pudo abrir la cámara.\")\n",
    "\n",
    "print(\"Cámara iniciada con éxito\")\n",
    "\n",
    "# Función para dibujar las cajas y verificar si se detectan casco, gafas y chaleco\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    person_id = 1  # Inicializamos el identificador para cada persona\n",
    "    helmet_detected = False\n",
    "    glasses_detected = False\n",
    "    vest_detected = False\n",
    "\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "\n",
    "        # Solo dibujamos cajas si la clase detectada es 'person' (clase 0 en COCO)\n",
    "        if c == 0:  # Clase 0 corresponde a 'person'\n",
    "            # Extraemos las coordenadas de la caja\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "\n",
    "            # Dibujamos la caja sobre el frame\n",
    "            color = colors[c]\n",
    "            cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 2)\n",
    "\n",
    "            # Etiquetamos a la persona como Persona 1, Persona 2, etc.\n",
    "            label = f\"Persona {person_id}\"\n",
    "            person_id += 1  # Incrementamos el contador para la siguiente persona\n",
    "\n",
    "            # Ponemos el texto de \"Persona: Sí\" en verde\n",
    "            cv2.putText(frame, \"Persona: Si\", (r[0], r[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Verificamos si hay casco, gafas y chaleco (suponiendo clases correspondientes)\n",
    "            for sub_box in boxes:\n",
    "                if int(sub_box.cls) == 1:  # Clase 1 corresponde a 'casco'\n",
    "                    helmet_detected = True\n",
    "                elif int(sub_box.cls) == 2:  # Clase 2 corresponde a 'gafas'\n",
    "                    glasses_detected = True\n",
    "                elif int(sub_box.cls) == 3:  # Clase 3 corresponde a 'chaleco'\n",
    "                    vest_detected = True\n",
    "\n",
    "            # Mostrar el estado del casco\n",
    "            if helmet_detected:\n",
    "                cv2.putText(frame, \"Casco: Si\", (r[0], r[1] + 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Casco: No\", (r[0], r[1] + 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            # Mostrar el estado de las gafas\n",
    "            if glasses_detected:\n",
    "                cv2.putText(frame, \"Gafas: Si\", (r[0], r[1] + 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Gafas: No\", (r[0], r[1] + 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            # Mostrar el estado del chaleco\n",
    "            if vest_detected:\n",
    "                cv2.putText(frame, \"Chaleco: Si\", (r[0], r[1] + 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Chaleco: No\", (r[0], r[1] + 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "# Bucle principal para capturar frames de la cámara y procesar las detecciones\n",
    "while cap.isOpened():\n",
    "    # Leemos un frame de la cámara\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"No se pudo leer el frame de la cámara.\")\n",
    "        break\n",
    "\n",
    "    # Invertimos el frame horizontalmente (opcional)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Realizamos la predicción con el modelo YOLO\n",
    "    results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "    # Procesamos cada resultado detectado\n",
    "    for result in results:\n",
    "        draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "    # Mostramos el frame con las detecciones en una ventana de OpenCV\n",
    "    cv2.imshow('Detección de Personas - Cámara', frame)\n",
    "\n",
    "    # Verificamos si se presiona la tecla 'q' para salir\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos la cámara y cerramos todas las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm246\\AppData\\Local\\Temp\\ipykernel_11704\\1926747168.py:40: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  imageF = imageF.resize((1280, 720), Image.ANTIALIAS)  # Redimensionar la imagen si es necesario\n"
     ]
    }
   ],
   "source": [
    "from tkinter import Tk, Label\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "\n",
    "def Scanning():\n",
    "\n",
    "    #leer Video\n",
    "    if cap is not None:\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "\n",
    "\n",
    "            #Resize\n",
    "            frame = imutils.resize(frame, width=640)\n",
    "\n",
    "            #Convertir video\n",
    "            im = Image.fromarray(frame)\n",
    "            img = ImageTk.PhotoImage(img=im)\n",
    "\n",
    "            #Mostrar\n",
    "            LblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "            lblVideo.after(10, Scanning)\n",
    "\n",
    "        else:\n",
    "            cap.release()\n",
    "    \n",
    "#main\n",
    "def ventana_principal():\n",
    "    global img_si_casco, img_si_gafas, img_si_chaleco, lblVideo\n",
    "    global img_no_casco, img_no_gafas, img_no_chaleco\n",
    "\n",
    "    # Ventana principal\n",
    "    pantalla = Tk()\n",
    "    pantalla.title(\"DETECCIÓN DE EPP EN EL ÁREA DE CONSTRUCCIÓN\")\n",
    "    pantalla.geometry(\"1280x720\")\n",
    "\n",
    "    # Background usando PIL\n",
    "    imageF = Image.open(\"Canva.png\")\n",
    "    imageF = imageF.resize((1280, 720), Image.ANTIALIAS)  # Redimensionar la imagen si es necesario\n",
    "    imageF = ImageTk.PhotoImage(imageF)\n",
    "    background = Label(pantalla, image=imageF)\n",
    "    background.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "\n",
    "    #model\n",
    "\n",
    "    #clases\n",
    "\n",
    "    #Img\n",
    "    img_si_casco = cv2.imread(\"Images/si_casco.png\")\n",
    "    img_si_gafas = cv2.imread(\"Images/si_gafas.png\")\n",
    "    img_si_chaleco = cv2.imread(\"Images/si_chaleco.png\")\n",
    "    img_no_casco = cv2.imread(\"Images/no_casco.png\")\n",
    "    img_no_gafas = cv2.imread(\"Images/no_gafas.png\")\n",
    "    img_no_chaleco = cv2.imread(\"Images/no_chaleco.png\")\n",
    "\n",
    "    # Mantener la referencia de la imagen para evitar que sea recolectada por el garbage collector\n",
    "    background.image = imageF\n",
    "\n",
    "    #label \n",
    "    lblVideo = Label(pantalla)\n",
    "    lblVideo.place(x=320, y=180)\n",
    "\n",
    "    #cam\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "    cap.set(3, 640)\n",
    "    cap.set(4, 480)\n",
    "\n",
    "    #Scannig\n",
    "    Scanning()\n",
    "\n",
    "\n",
    "    # Loop\n",
    "    pantalla.mainloop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ventana_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cámara iniciada con éxito\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# Cargar tu modelo YOLO personalizado (reemplaza la ruta con la de tu modelo)\n",
    "model = YOLO(\"best.pt\")  # Carga tu modelo entrenado\n",
    "\n",
    "# Definir las clases que entrenaste en tu modelo (por ejemplo: casco, gafas, chaleco)\n",
    "classes = [\"Persona\", \"Casco\", \"Gafas\", \"Chaleco\"]  # Asegúrate de que coincidan con tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Iniciamos la captura de video desde la cámara (índice 1 para la cámara predeterminada)\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Verificamos si la cámara se ha iniciado correctamente\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"No se pudo abrir la cámara.\")\n",
    "\n",
    "print(\"Cámara iniciada con éxito\")\n",
    "\n",
    "# Función para dibujar las cajas y verificar si se detectan casco, gafas y chaleco\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    person_id = 1  # Inicializamos el identificador para cada persona\n",
    "    helmet_detected = False\n",
    "    glasses_detected = False\n",
    "    vest_detected = False\n",
    "\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "\n",
    "        # Solo dibujamos cajas si la clase detectada es 'person' (clase 0 en tu dataset)\n",
    "        if c == 0:  # Clase 0 corresponde a 'persona'\n",
    "            # Extraemos las coordenadas de la caja\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "\n",
    "            # Dibujamos la caja sobre el frame\n",
    "            color = colors[c]\n",
    "            cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 2)\n",
    "\n",
    "            # Etiquetamos a la persona como Persona 1, Persona 2, etc.\n",
    "            label = f\"Persona {person_id}\"\n",
    "            person_id += 1  # Incrementamos el contador para la siguiente persona\n",
    "\n",
    "            # Ponemos el texto de \"Persona: Sí\" en verde\n",
    "            cv2.putText(frame, \"Persona: Si\", (r[0], r[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Verificamos si hay casco, gafas y chaleco (suponiendo clases 1, 2, 3 en tu modelo)\n",
    "            for sub_box in boxes:\n",
    "                if int(sub_box.cls) == 1:  # Clase 1 corresponde a 'casco'\n",
    "                    helmet_detected = True\n",
    "                elif int(sub_box.cls) == 2:  # Clase 2 corresponde a 'gafas'\n",
    "                    glasses_detected = True\n",
    "                elif int(sub_box.cls) == 3:  # Clase 3 corresponde a 'chaleco'\n",
    "                    vest_detected = True\n",
    "\n",
    "            # Mostrar el estado del casco\n",
    "            if helmet_detected:\n",
    "                cv2.putText(frame, \"Casco: Si\", (r[0], r[1] + 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Casco: No\", (r[0], r[1] + 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            # Mostrar el estado de las gafas\n",
    "            if glasses_detected:\n",
    "                cv2.putText(frame, \"Gafas: Si\", (r[0], r[1] + 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Gafas: No\", (r[0], r[1] + 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            # Mostrar el estado del chaleco\n",
    "            if vest_detected:\n",
    "                cv2.putText(frame, \"Chaleco: Si\", (r[0], r[1] + 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Chaleco: No\", (r[0], r[1] + 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "# Bucle principal para capturar frames de la cámara y procesar las detecciones\n",
    "while cap.isOpened():\n",
    "    # Leemos un frame de la cámara\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"No se pudo leer el frame de la cámara.\")\n",
    "        break\n",
    "\n",
    "    # Invertimos el frame horizontalmente (opcional)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Realizamos la predicción con tu modelo entrenado YOLO\n",
    "    results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "    # Procesamos cada resultado detectado\n",
    "    for result in results:\n",
    "        draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "    # Mostramos el frame con las detecciones en una ventana de OpenCV\n",
    "    cv2.imshow('Detección de EPP - Cámara', frame)\n",
    "\n",
    "    # Verificamos si se presiona la tecla 'q' para salir\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos la cámara y cerramos todas las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\jm246\\appdata\\roaming\\python\\python311\\site-packages (8.3.12)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (1.23.5)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (3.7.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\jm246\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (9.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\jm246\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\jm246\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (4.64.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\jm246\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (5.9.4)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (1.5.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\jm246\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (2.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jm246\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jm246\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jm246\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jm246\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jm246\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cámara iniciada con éxito\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "\n",
    "# Cargar tu modelo YOLO personalizado\n",
    "model = YOLO(\"best.pt\")  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = [\"Persona\", \"Casco\", \"Gafas\", \"Chaleco\"]  # Asegúrate de que coincidan con tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Iniciamos la captura de video desde la cámara (índice 1 para la cámara predeterminada)\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Verificamos si la cámara se ha iniciado correctamente\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"No se pudo abrir la cámara.\")\n",
    "\n",
    "print(\"Cámara iniciada con éxito\")\n",
    "\n",
    "# Función para dibujar las cajas y verificar si se detectan diferentes equipos de protección personal\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    person_id = 1  # Inicializamos el identificador para cada persona\n",
    "\n",
    "    # Crear un diccionario para almacenar los EPP detectados por persona\n",
    "    persons_epp = {}\n",
    "\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "\n",
    "        # Verificar si la clase detectada está dentro del rango de clases definidas\n",
    "        if c >= len(labels):\n",
    "            # Si la clase detectada no está dentro de la lista, la ignoramos\n",
    "            continue\n",
    "\n",
    "        # Extraemos las coordenadas de la caja\n",
    "        r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "        r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "        color = colors[c]\n",
    "\n",
    "        if labels[c] == \"Persona\":  # Detectamos la persona\n",
    "            person_id_str = f\"Persona {person_id}\"\n",
    "            persons_epp[person_id_str] = {\"casco\": False, \"gafas\": False, \"chaleco\": False}\n",
    "            # Dibujar el rectángulo y la etiqueta para la persona\n",
    "            cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 2)\n",
    "            cv2.putText(frame, person_id_str, (r[0], r[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            person_id += 1\n",
    "\n",
    "        # Identificar los EPP en relación con la persona detectada\n",
    "        elif labels[c] in [\"Casco\", \"Gafas\", \"Chaleco\"]:\n",
    "            # Buscamos la persona más cercana al EPP detectado\n",
    "            for person_label, epp_status in persons_epp.items():\n",
    "                # Comprobar si el EPP se encuentra en la misma área que la persona\n",
    "                # Nota: Esta comprobación es muy simple y puede ser mejorada\n",
    "                if r[0] >= 0 and r[1] >= 0:  # Si el EPP está dentro del área visible\n",
    "                    if labels[c] == \"Casco\":\n",
    "                        epp_status[\"casco\"] = True\n",
    "                    elif labels[c] == \"Gafas\":\n",
    "                        epp_status[\"gafas\"] = True\n",
    "                    elif labels[c] == \"Chaleco\":\n",
    "                        epp_status[\"chaleco\"] = True\n",
    "\n",
    "            # Dibujar el rectángulo y la etiqueta para el EPP\n",
    "            label = f\"{labels[c]} {box.conf[0]:.2f}\"\n",
    "            cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 2)\n",
    "            cv2.putText(frame, label, (r[0], r[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "    # Mostrar el estado de los EPP para cada persona\n",
    "    for person_label, epp_status in persons_epp.items():\n",
    "        y_offset = 30\n",
    "        for epp, detected in epp_status.items():\n",
    "            color = (0, 255, 0) if detected else (0, 0, 255)\n",
    "            cv2.putText(frame, f\"{person_label} - {epp.capitalize()}: {'Si' if detected else 'No'}\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            y_offset += 30\n",
    "\n",
    "# Bucle principal para capturar frames de la cámara y procesar las detecciones\n",
    "while cap.isOpened():\n",
    "    # Leemos un frame de la cámara\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"No se pudo leer el frame de la cámara.\")\n",
    "        break\n",
    "\n",
    "    # Invertimos el frame horizontalmente (opcional)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Realizamos la predicción con tu modelo entrenado YOLO\n",
    "    results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "    # Procesamos cada resultado detectado\n",
    "    for result in results:\n",
    "        draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "    # Mostramos el frame con las detecciones en una ventana de OpenCV\n",
    "    cv2.imshow('Detección de EPP - Cámara', frame)\n",
    "\n",
    "    # Verificamos si se presiona la tecla 'q' para salir\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos la cámara y cerramos todas las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cámara iniciada con éxito\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "\n",
    "# Cargar tu modelo YOLO personalizado\n",
    "model = YOLO(\"best.pt\")  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = [\"arnes\", \"botas\", \"casco\", \"chalecos\", \"guantes\", \"lentes\", \"persona\"]  # Clases de tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Iniciamos la captura de video desde la cámara (índice 1 para la cámara predeterminada)\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Verificamos si la cámara se ha iniciado correctamente\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"No se pudo abrir la cámara.\")\n",
    "\n",
    "print(\"Cámara iniciada con éxito\")\n",
    "\n",
    "# Función para dibujar las cajas y verificar si se detectan diferentes equipos de protección personal\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "        \n",
    "        # Extraemos las coordenadas de la caja\n",
    "        r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "        r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "\n",
    "        # Dibujamos la caja sobre el frame con un borde grueso (4 píxeles)\n",
    "        color = colors[c]\n",
    "        cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 4)\n",
    "\n",
    "        # Etiqueta con el nombre de la clase y el puntaje\n",
    "        label = f\"{classes[c]} {box.conf[0]:.2f}\"\n",
    "\n",
    "        # Definir el tamaño del texto y la ubicación\n",
    "        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "        \n",
    "        # Crear un fondo sólido para la etiqueta\n",
    "        cv2.rectangle(frame, (r[0], r[1] - h - 10), (r[0] + w, r[1]), color, -1)\n",
    "\n",
    "        # Colocar el texto sobre el fondo\n",
    "        cv2.putText(frame, label, (r[0], r[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "# Bucle principal para capturar frames de la cámara y procesar las detecciones\n",
    "while cap.isOpened():\n",
    "    # Leemos un frame de la cámara\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"No se pudo leer el frame de la cámara.\")\n",
    "        break\n",
    "\n",
    "    # Invertimos el frame horizontalmente (opcional)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Realizamos la predicción con tu modelo entrenado YOLO\n",
    "    results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "    # Procesamos cada resultado detectado\n",
    "    for result in results:\n",
    "        draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "    # Mostramos el frame con las detecciones en una ventana de OpenCV\n",
    "    cv2.imshow('Detección de EPP - Cámara', frame)\n",
    "\n",
    "    # Verificamos si se presiona la tecla 'q' para salir\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos la cámara y cerramos todas las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando video ppe-1.mp4...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "\n",
    "# Cargar tu modelo YOLO personalizado\n",
    "model = YOLO(\"best.pt\")  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = [\"arnes\", \"botas\", \"casco\", \"chalecos\", \"guantes\", \"lentes\", \"persona\"]  # Clases de tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Cargar el video en lugar de usar la cámara\n",
    "video_path = \"ppe-1.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Verificamos si el video se ha cargado correctamente\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"No se pudo abrir el archivo de video.\")\n",
    "\n",
    "# Obtener la información del video (ancho, alto, FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# (Opcional) Guardar el video procesado\n",
    "output_video_path = \"ppe-1_output.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec para el video\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "print(f\"Procesando video {video_path}...\")\n",
    "\n",
    "# Función para dibujar las cajas y verificar si se detectan diferentes equipos de protección personal\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "        \n",
    "        # Extraemos las coordenadas de la caja\n",
    "        r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "        r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "\n",
    "        # Dibujamos la caja sobre el frame con un borde grueso (4 píxeles)\n",
    "        color = colors[c]\n",
    "        cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 4)\n",
    "\n",
    "        # Etiqueta con el nombre de la clase y el puntaje\n",
    "        label = f\"{classes[c]} {box.conf[0]:.2f}\"\n",
    "\n",
    "        # Definir el tamaño del texto y la ubicación\n",
    "        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "        \n",
    "        # Crear un fondo sólido para la etiqueta\n",
    "        cv2.rectangle(frame, (r[0], r[1] - h - 10), (r[0] + w, r[1]), color, -1)\n",
    "\n",
    "        # Colocar el texto sobre el fondo\n",
    "        cv2.putText(frame, label, (r[0], r[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "# Bucle principal para capturar frames del video y procesar las detecciones\n",
    "while cap.isOpened():\n",
    "    # Leemos un frame del video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"No se pudo leer el frame o se ha terminado el video.\")\n",
    "        break\n",
    "\n",
    "    # Realizamos la predicción con tu modelo entrenado YOLO\n",
    "    results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "    # Procesamos cada resultado detectado\n",
    "    for result in results:\n",
    "        draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "    # Mostramos el frame con las detecciones en una ventana de OpenCV\n",
    "    cv2.imshow('Detección de EPP - Video', frame)\n",
    "\n",
    "    # (Opcional) Guardamos el frame procesado en el video de salida\n",
    "    out.write(frame)\n",
    "\n",
    "    # Verificamos si se presiona la tecla 'q' para salir\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos el video y cerramos todas las ventanas\n",
    "cap.release()\n",
    "out.release()  # Liberamos el video de salida\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando video video.mp4...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "\n",
    "# Cargar tu modelo YOLO personalizado\n",
    "model = YOLO(\"best.pt\")  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = [\"arnes\", \"botas\", \"casco\", \"chalecos\", \"guantes\", \"lentes\", \"persona\"]  # Clases de tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Cargar el video en lugar de usar la cámara\n",
    "video_path = \"video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Verificamos si el video se ha cargado correctamente\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"No se pudo abrir el archivo de video.\")\n",
    "\n",
    "# Obtener la información del video (ancho, alto, FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# (Opcional) Guardar el video procesado\n",
    "output_video_path = \"video_f.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec para el video\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "print(f\"Procesando video {video_path}...\")\n",
    "\n",
    "# Función para dibujar las cajas y verificar si se detectan diferentes equipos de protección personal\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    people_boxes = []\n",
    "    epp_detections = {\"chaleco\": [], \"casco\": [], \"lentes\": [], \"botas\": [], \"arnes\": []}\n",
    "\n",
    "    # Primer paso: detectar personas y guardar sus coordenadas\n",
    "    for box in boxes:\n",
    "        c = int(box.cls)\n",
    "        if c == 6:  # Clase 6 corresponde a 'persona'\n",
    "            people_boxes.append(box.xyxy[0].cpu().numpy())  # Guardamos las coordenadas de la persona\n",
    "        elif c == 3:  # Clase 3 corresponde a 'chalecos'\n",
    "            epp_detections[\"chaleco\"].append(box.xyxy[0].cpu().numpy())\n",
    "        elif c == 2:  # Clase 2 corresponde a 'casco'\n",
    "            epp_detections[\"casco\"].append(box.xyxy[0].cpu().numpy())\n",
    "        elif c == 5:  # Clase 5 corresponde a 'lentes'\n",
    "            epp_detections[\"lentes\"].append(box.xyxy[0].cpu().numpy())\n",
    "        elif c == 1:  # Clase 1 corresponde a 'botas'\n",
    "            epp_detections[\"botas\"].append(box.xyxy[0].cpu().numpy())\n",
    "        elif c == 0:  # Clase 0 corresponde a 'arnes'\n",
    "            epp_detections[\"arnes\"].append(box.xyxy[0].cpu().numpy())\n",
    "\n",
    "    # Segundo paso: para cada persona detectada, buscamos el EPP más cercano\n",
    "    for person_box in people_boxes:\n",
    "        r = [int(coord) for coord in person_box]  # Convertimos a enteros\n",
    "        color = COLORS[6]  # Color para 'persona'\n",
    "        cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 4)  # Dibujamos la caja\n",
    "\n",
    "        # Etiqueta para la persona\n",
    "        label = \"Persona\"\n",
    "        cv2.putText(frame, label, (r[0], r[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "        # Buscar EPPs asociados a esta persona\n",
    "        # Asignamos el EPP más cercano basado en la distancia al centro de la caja de la persona\n",
    "        centro_persona = np.array([(r[0] + r[2]) / 2, (r[1] + r[3]) / 2])\n",
    "\n",
    "        def find_closest_epp(epp_list):\n",
    "            \"\"\" Encuentra el EPP más cercano a la persona \"\"\"\n",
    "            if not epp_list:\n",
    "                return False\n",
    "            closest_epp = min(epp_list, key=lambda epp: np.linalg.norm(centro_persona - [(epp[0] + epp[2]) / 2, (epp[1] + epp[3]) / 2]))\n",
    "            epp_distance = np.linalg.norm(centro_persona - [(closest_epp[0] + closest_epp[2]) / 2, (closest_epp[1] + closest_epp[3]) / 2])\n",
    "            # Consideramos una distancia límite para asociar el EPP con la persona\n",
    "            if epp_distance < 150:  # Puedes ajustar este valor según tu video\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        # Verificamos si la persona tiene chaleco, casco, lentes, botas, arnés\n",
    "        chaleco = find_closest_epp(epp_detections[\"chaleco\"])\n",
    "        casco = find_closest_epp(epp_detections[\"casco\"])\n",
    "        lentes = find_closest_epp(epp_detections[\"lentes\"])\n",
    "        botas = find_closest_epp(epp_detections[\"botas\"])\n",
    "        arnes = find_closest_epp(epp_detections[\"arnes\"])\n",
    "\n",
    "        # Mostrar el estado del EPP\n",
    "        cv2.putText(frame, f\"Chaleco: {'Si' if chaleco else 'No'}\", (r[0], r[1] + 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if chaleco else (0, 0, 255), 2)\n",
    "        cv2.putText(frame, f\"Casco: {'Si' if casco else 'No'}\", (r[0], r[1] + 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if casco else (0, 0, 255), 2)\n",
    "        cv2.putText(frame, f\"Lentes: {'Si' if lentes else 'No'}\", (r[0], r[1] + 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if lentes else (0, 0, 255), 2)\n",
    "        cv2.putText(frame, f\"Botas: {'Si' if botas else 'No'}\", (r[0], r[1] + 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if botas else (0, 0, 255), 2)\n",
    "        cv2.putText(frame, f\"Arnes: {'Si' if arnes else 'No'}\", (r[0], r[1] + 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if arnes else (0, 0, 255), 2)\n",
    "\n",
    "# Bucle principal para capturar frames del video y procesar las detecciones\n",
    "while cap.isOpened():\n",
    "    # Leemos un frame del video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"No se pudo leer el frame o se ha terminado el video.\")\n",
    "        break\n",
    "\n",
    "    # Realizamos la predicción con tu modelo entrenado YOLO\n",
    "    results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "    # Procesamos cada resultado detectado\n",
    "    for result in results:\n",
    "        draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "    # Mostramos el frame con las detecciones en una ventana de OpenCV\n",
    "    cv2.imshow('Detección de EPP - Video', frame)\n",
    "\n",
    "    # (Opcional) Guardamos el frame procesado en el video de salida\n",
    "    out.write(frame)\n",
    "\n",
    "    # Verificamos si se presiona la tecla 'q' para salir\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos el video y cerramos todas las ventanas\n",
    "cap.release()\n",
    "out.release()  # Liberamos el video de salida\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video cargado con éxito, procesando...\n",
      "No se pudo leer el frame o se ha terminado el video.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "\n",
    "# Cargar tu modelo YOLO personalizado\n",
    "model = YOLO(\"best1.pt\")  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = [\"arnes\", \"botas\", \"casco\", \"chalecos\", \"guantes\", \"lentes\", \"persona\"]  # Clases de tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Cargar el video en lugar de usar la cámara\n",
    "video_path = \"ppe-1.mp4\"  # Ruta de tu video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Verificamos si el video se ha cargado correctamente\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"No se pudo abrir el archivo de video.\")\n",
    "\n",
    "# Obtener la información del video (ancho, alto, FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# (Opcional) Guardar el video procesado\n",
    "output_video_path = \"ppe-1_output.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec para el video\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "print(\"Video cargado con éxito, procesando...\")\n",
    "\n",
    "# Función para dibujar las cajas y verificar si se detectan diferentes equipos de protección personal\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    person_id = 1  # Inicializamos el identificador para cada persona\n",
    "    helmet_detected = False\n",
    "    vest_detected = False\n",
    "    gloves_detected = False\n",
    "    harness_detected = False\n",
    "    boots_detected = False\n",
    "    glasses_detected = False\n",
    "\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "\n",
    "        # Solo dibujamos cajas si la clase detectada es 'persona' (clase 6 en tu dataset)\n",
    "        if c == 6:  # Clase 6 corresponde a 'persona'\n",
    "            # Extraemos las coordenadas de la caja\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "\n",
    "            # Dibujamos la caja sobre el frame\n",
    "            color = COLORS[c]\n",
    "            cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 2)\n",
    "\n",
    "            # Etiquetamos a la persona como Persona 1, Persona 2, etc.\n",
    "            label = f\"Persona {person_id}\"\n",
    "            person_id += 1  # Incrementamos el contador para la siguiente persona\n",
    "\n",
    "            # Ponemos el texto de \"Persona: Sí\" en verde\n",
    "            cv2.putText(frame, \"Persona: Si\", (r[0], r[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Verificamos si se detectan los EPPs: arnes, botas, casco, chalecos, guantes, lentes\n",
    "            for sub_box in boxes:\n",
    "                if int(sub_box.cls) == 0:  # Clase 0 corresponde a 'arnes'\n",
    "                    harness_detected = True\n",
    "                elif int(sub_box.cls) == 1:  # Clase 1 corresponde a 'botas'\n",
    "                    boots_detected = True\n",
    "                elif int(sub_box.cls) == 2:  # Clase 2 corresponde a 'casco'\n",
    "                    helmet_detected = True\n",
    "                elif int(sub_box.cls) == 3:  # Clase 3 corresponde a 'chalecos'\n",
    "                    vest_detected = True\n",
    "                elif int(sub_box.cls) == 4:  # Clase 4 corresponde a 'guantes'\n",
    "                    gloves_detected = True\n",
    "                elif int(sub_box.cls) == 5:  # Clase 5 corresponde a 'lentes'\n",
    "                    glasses_detected = True\n",
    "\n",
    "            # Mostrar el estado de cada EPP\n",
    "            cv2.putText(frame, f\"Arnes: {'Si' if harness_detected else 'No'}\", (r[0], r[1] + 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if harness_detected else (0, 0, 255), 2)\n",
    "            cv2.putText(frame, f\"Botas: {'Si' if boots_detected else 'No'}\", (r[0], r[1] + 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if boots_detected else (0, 0, 255), 2)\n",
    "            cv2.putText(frame, f\"Casco: {'Si' if helmet_detected else 'No'}\", (r[0], r[1] + 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if helmet_detected else (0, 0, 255), 2)\n",
    "            cv2.putText(frame, f\"Chaleco: {'Si' if vest_detected else 'No'}\", (r[0], r[1] + 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if vest_detected else (0, 0, 255), 2)\n",
    "            cv2.putText(frame, f\"Guantes: {'Si' if gloves_detected else 'No'}\", (r[0], r[1] + 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if gloves_detected else (0, 0, 255), 2)\n",
    "            cv2.putText(frame, f\"Lentes: {'Si' if glasses_detected else 'No'}\", (r[0], r[1] + 180), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if glasses_detected else (0, 0, 255), 2)\n",
    "\n",
    "# Bucle principal para capturar frames del video y procesar las detecciones\n",
    "while cap.isOpened():\n",
    "    # Leemos un frame del video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"No se pudo leer el frame o se ha terminado el video.\")\n",
    "        break\n",
    "\n",
    "    # Realizamos la predicción con tu modelo entrenado YOLO\n",
    "    results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "    # Procesamos cada resultado detectado\n",
    "    for result in results:\n",
    "        draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "    # Mostramos el frame con las detecciones en una ventana de OpenCV\n",
    "    cv2.imshow('Detección de EPP - Video', frame)\n",
    "\n",
    "    # (Opcional) Guardamos el frame procesado en el video de salida\n",
    "    out.write(frame)\n",
    "\n",
    "    # Verificamos si se presiona la tecla 'q' para salir\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos el video y cerramos todas las ventanas\n",
    "cap.release()\n",
    "out.release()  # Liberamos el video de salida\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm246\\AppData\\Local\\Temp\\ipykernel_12548\\3465872260.py:47: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  imageF = imageF.resize((1280, 720), Image.ANTIALIAS)  # Redimensionar la imagen si es necesario\n"
     ]
    }
   ],
   "source": [
    "from tkinter import Tk, Label\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import imutils\n",
    "\n",
    "# Definir la cámara fuera de las funciones para que sea accesible en todo el código\n",
    "cap = None\n",
    "\n",
    "def Scanning():\n",
    "    global cap, lblVideo\n",
    "\n",
    "    # Leer el video de la cámara\n",
    "    if cap is not None:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Invertir la imagen horizontalmente (efecto espejo)\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Redimensionar el frame de video\n",
    "            frame = imutils.resize(frame, width=640)\n",
    "\n",
    "            # Convertir el frame de BGR (OpenCV) a RGB (PIL)\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            im = Image.fromarray(frame_rgb)\n",
    "            img = ImageTk.PhotoImage(im)\n",
    "\n",
    "            # Mostrar la imagen en el label\n",
    "            lblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "\n",
    "            # Llamar a la función de nuevo después de 10ms para crear el efecto de video\n",
    "            lblVideo.after(10, Scanning)\n",
    "        else:\n",
    "            cap.release()\n",
    "\n",
    "# Función principal para crear la ventana de Tkinter\n",
    "def ventana_principal():\n",
    "    global cap, lblVideo\n",
    "\n",
    "    # Crear la ventana principal\n",
    "    pantalla = Tk()\n",
    "    pantalla.title(\"DETECCIÓN DE EPP EN EL ÁREA DE CONSTRUCCIÓN\")\n",
    "    pantalla.geometry(\"1280x720\")\n",
    "\n",
    "    # Background usando PIL\n",
    "    imageF = Image.open(\"Canva.png\")\n",
    "    imageF = imageF.resize((1280, 720), Image.ANTIALIAS)  # Redimensionar la imagen si es necesario\n",
    "    imageF = ImageTk.PhotoImage(imageF)\n",
    "    background = Label(pantalla, image=imageF)\n",
    "    background.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "\n",
    "    # Mantener la referencia de la imagen para evitar que sea recolectada por el garbage collector\n",
    "    background.image = imageF\n",
    "\n",
    "    # Label para mostrar el video\n",
    "    lblVideo = Label(pantalla)\n",
    "    lblVideo.place(x=320, y=120)\n",
    "\n",
    "    # Configurar la cámara\n",
    "    cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "    cap.set(3, 640)  # Ancho de la cámara\n",
    "    cap.set(4, 480)  # Alto de la cámara\n",
    "\n",
    "    # Iniciar la función de escaneo de la cámara\n",
    "    Scanning()\n",
    "\n",
    "    # Iniciar el loop principal de la ventana de Tkinter\n",
    "    pantalla.mainloop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ventana_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm246\\AppData\\Local\\Temp\\ipykernel_12712\\3888081961.py:102: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "from tkinter import Tk, Label\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Definir el modelo YOLO como global\n",
    "model = YOLO('best.pt')  # Cambia 'best.pt' por la ruta a tu modelo si está en otro lugar\n",
    "clsName = [\"casco\", \"gafas\", \"chaleco\"]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(clsName), 3))\n",
    "\n",
    "# Definir la cámara fuera de las funciones para acceso global\n",
    "cap = None\n",
    "\n",
    "def update_epp_status(casco_detected, gafas_detected, chaleco_detected):\n",
    "    # Actualiza las imágenes según las detecciones\n",
    "    if casco_detected:\n",
    "        lbl_casco.config(image=img_si_casco)\n",
    "        lbl_casco.image = img_si_casco\n",
    "    else:\n",
    "        lbl_casco.config(image=img_no_casco)\n",
    "        lbl_casco.image = img_no_casco\n",
    "\n",
    "    if gafas_detected:\n",
    "        lbl_gafas.config(image=img_si_gafas)\n",
    "        lbl_gafas.image = img_si_gafas\n",
    "    else:\n",
    "        lbl_gafas.config(image=img_no_gafas)\n",
    "        lbl_gafas.image = img_no_gafas\n",
    "\n",
    "    if chaleco_detected:\n",
    "        lbl_chaleco.config(image=img_si_chaleco)\n",
    "        lbl_chaleco.image = img_si_chaleco\n",
    "    else:\n",
    "        lbl_chaleco.config(image=img_no_chaleco)\n",
    "        lbl_chaleco.image = img_no_chaleco\n",
    "\n",
    "def draw_boxes(frame, boxes):\n",
    "    casco_detected = gafas_detected = chaleco_detected = False\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "        cls = int(box.cls[0])\n",
    "        conf = box.conf[0]\n",
    "\n",
    "        # Verificar si la clase detectada está dentro de los índices válidos\n",
    "        if cls < len(clsName):\n",
    "            # Determinar si se detectó cada EPP\n",
    "            if cls == 0:  # Casco\n",
    "                casco_detected = True\n",
    "            elif cls == 1:  # Gafas\n",
    "                gafas_detected = True\n",
    "            elif cls == 2:  # Chaleco\n",
    "                chaleco_detected = True\n",
    "\n",
    "            # Dibujar el rectángulo y el texto\n",
    "            color = COLORS[cls]\n",
    "            label = f\"{clsName[cls]} {conf:.2f}\"\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    # Actualizar la interfaz con el estado de los EPP detectados\n",
    "    update_epp_status(casco_detected, gafas_detected, chaleco_detected)\n",
    "\n",
    "def Scanning():\n",
    "    global cap  # Declarar cap como global para asegurarnos de tener acceso a la cámara\n",
    "\n",
    "    if cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "            for result in results:\n",
    "                draw_boxes(frame, result.boxes)\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(frame_rgb))\n",
    "            lblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "\n",
    "        lblVideo.after(10, Scanning)\n",
    "\n",
    "def ventana_principal():\n",
    "    global cap, lblVideo, lbl_casco, lbl_gafas, lbl_chaleco\n",
    "    global img_no_casco, img_no_gafas, img_no_chaleco\n",
    "    global img_si_casco, img_si_gafas, img_si_chaleco\n",
    "\n",
    "    pantalla = Tk()\n",
    "    pantalla.title(\"Detección de EPP\")\n",
    "    pantalla.geometry(\"1280x720\")\n",
    "\n",
    "    # Cargar la imagen de fondo\n",
    "    try:\n",
    "        imageF = Image.open(\"Canva.png\")\n",
    "        imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n",
    "        imageF = ImageTk.PhotoImage(imageF)\n",
    "        background = Label(pantalla, image=imageF)\n",
    "        background.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "        background.image = imageF\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: No se pudo cargar 'Canva.png'. Asegúrate de que la imagen esté en la misma carpeta.\")\n",
    "\n",
    "    # Configurar las rutas de las imágenes\n",
    "    base_path = os.path.abspath(\"Images\")\n",
    "\n",
    "    img_no_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_casco - copia.png\")))\n",
    "    img_no_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_gafas - copia.png\")))\n",
    "    img_no_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_chaleco - copia.png\")))\n",
    "    img_si_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_casco - copia.png\")))\n",
    "    img_si_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_gafas - copia.png\")))\n",
    "    img_si_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_chaleco - copia.png\")))\n",
    "\n",
    "    # Etiquetas para mostrar las imágenes de estado de EPP en la interfaz\n",
    "    lbl_casco = Label(pantalla, image=img_no_casco, bg='#d9d9d9')  # Fondo gris claro para que destaque\n",
    "    lbl_casco.place(x=75, y=110)  # Ajustar hacia abajo (y+10)\n",
    "    lbl_gafas = Label(pantalla, image=img_no_gafas, bg='#d9d9d9')\n",
    "    lbl_gafas.place(x=75, y=260)  # Ajustar hacia abajo (y+10)\n",
    "    lbl_chaleco = Label(pantalla, image=img_no_chaleco, bg='#d9d9d9')\n",
    "    lbl_chaleco.place(x=75, y=410)  # Ajustar hacia abajo (y+10)\n",
    "\n",
    "    # Configuración del video\n",
    "    lblVideo = Label(pantalla)\n",
    "    lblVideo.place(x=320, y=120)\n",
    "\n",
    "    # Configurar la cámara\n",
    "    cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # Intentar con la cámara 0 si la 1 no funciona\n",
    "    cap.set(3, 640)\n",
    "    cap.set(4, 480)\n",
    "\n",
    "    # Iniciar la detección\n",
    "    Scanning()\n",
    "    pantalla.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ventana_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm246\\AppData\\Local\\Temp\\ipykernel_10580\\1958830124.py:99: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "from tkinter import Tk, Label\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Definir el modelo YOLO como global\n",
    "model = YOLO('best.pt')  # Cambia 'best.pt' por la ruta a tu modelo si está en otro lugar\n",
    "clsName = [\"casco\", \"gafas\", \"chaleco\", \"persona\"]  # Se agregó la clase \"persona\"\n",
    "COLORS = np.random.uniform(0, 255, size=(len(clsName), 3))\n",
    "\n",
    "# Definir la cámara fuera de las funciones para acceso global\n",
    "cap = None\n",
    "\n",
    "def update_epp_status(casco_detected, gafas_detected, chaleco_detected):\n",
    "    # Actualiza las imágenes según las detecciones de EPP\n",
    "    if casco_detected:\n",
    "        lbl_casco.config(image=img_si_casco)\n",
    "        lbl_casco.image = img_si_casco\n",
    "    else:\n",
    "        lbl_casco.config(image=img_no_casco)\n",
    "        lbl_casco.image = img_no_casco\n",
    "\n",
    "    if gafas_detected:\n",
    "        lbl_gafas.config(image=img_si_gafas)\n",
    "        lbl_gafas.image = img_si_gafas\n",
    "    else:\n",
    "        lbl_gafas.config(image=img_no_gafas)\n",
    "        lbl_gafas.image = img_no_gafas\n",
    "\n",
    "    if chaleco_detected:\n",
    "        lbl_chaleco.config(image=img_si_chaleco)\n",
    "        lbl_chaleco.image = img_si_chaleco\n",
    "    else:\n",
    "        lbl_chaleco.config(image=img_no_chaleco)\n",
    "        lbl_chaleco.image = img_no_chaleco\n",
    "\n",
    "def draw_boxes(frame, boxes):\n",
    "    casco_detected = gafas_detected = chaleco_detected = persona_detected = False\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "        cls = int(box.cls[0])\n",
    "        conf = box.conf[0]\n",
    "\n",
    "        # Verificar si la clase detectada está dentro de los índices válidos\n",
    "        if cls < len(clsName):\n",
    "            # Determinar si se detectó cada clase\n",
    "            if cls == 0:  # Casco\n",
    "                casco_detected = True\n",
    "            elif cls == 1:  # Gafas\n",
    "                gafas_detected = True\n",
    "            elif cls == 2:  # Chaleco\n",
    "                chaleco_detected = True\n",
    "            elif cls == 3:  # Persona\n",
    "                persona_detected = True  # Se detectó una persona\n",
    "\n",
    "            # Dibujar el rectángulo y el texto para cada detección\n",
    "            color = COLORS[cls]\n",
    "            label = f\"{clsName[cls]} {conf:.2f}\"\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    # Actualizar la interfaz con el estado de los EPP detectados\n",
    "    update_epp_status(casco_detected, gafas_detected, chaleco_detected)\n",
    "\n",
    "def Scanning():\n",
    "    global cap  # Declarar cap como global para asegurarnos de tener acceso a la cámara\n",
    "\n",
    "    if cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "            for result in results:\n",
    "                draw_boxes(frame, result.boxes)\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(frame_rgb))\n",
    "            lblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "\n",
    "        lblVideo.after(10, Scanning)\n",
    "\n",
    "def ventana_principal():\n",
    "    global cap, lblVideo, lbl_casco, lbl_gafas, lbl_chaleco\n",
    "    global img_no_casco, img_no_gafas, img_no_chaleco\n",
    "    global img_si_casco, img_si_gafas, img_si_chaleco\n",
    "\n",
    "    pantalla = Tk()\n",
    "    pantalla.title(\"Detección de EPP\")\n",
    "    pantalla.geometry(\"1280x720\")\n",
    "\n",
    "    # Cargar la imagen de fondo\n",
    "    try:\n",
    "        imageF = Image.open(\"Canva.png\")\n",
    "        imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n",
    "        imageF = ImageTk.PhotoImage(imageF)\n",
    "        background = Label(pantalla, image=imageF)\n",
    "        background.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "        background.image = imageF\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: No se pudo cargar 'Canva.png'. Asegúrate de que la imagen esté en la misma carpeta.\")\n",
    "\n",
    "    # Configurar las rutas de las imágenes de EPP\n",
    "    base_path = os.path.abspath(\"Images\")\n",
    "\n",
    "    img_no_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_casco.png\")))\n",
    "    img_no_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_gafas.png\")))\n",
    "    img_no_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_chaleco.png\")))\n",
    "    img_si_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_casco.png\")))\n",
    "    img_si_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_gafas.png\")))\n",
    "    img_si_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_chaleco.png\")))\n",
    "\n",
    "    # Etiquetas para mostrar las imágenes de estado de EPP en la interfaz\n",
    "    lbl_casco = Label(pantalla, image=img_no_casco, bg='#d9d9d9')  # Fondo gris claro para que destaque\n",
    "    lbl_casco.place(x=75, y=150)\n",
    "    lbl_gafas = Label(pantalla, image=img_no_gafas, bg='#d9d9d9')\n",
    "    lbl_gafas.place(x=75, y=300)\n",
    "    lbl_chaleco = Label(pantalla, image=img_no_chaleco, bg='#d9d9d9')\n",
    "    lbl_chaleco.place(x=75, y=450)\n",
    "\n",
    "    # Configuración del video\n",
    "    lblVideo = Label(pantalla)\n",
    "    lblVideo.place(x=320, y=120)\n",
    "\n",
    "    # Configurar la cámara\n",
    "    cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # Intentar con la cámara 0 si la 1 no funciona\n",
    "    cap.set(3, 640)\n",
    "    cap.set(4, 480)\n",
    "\n",
    "    # Iniciar la detección\n",
    "    Scanning()\n",
    "    pantalla.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ventana_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm246\\AppData\\Local\\Temp\\ipykernel_17580\\2210658182.py:102: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "from tkinter import Tk, Label\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Definir el modelo YOLO como global\n",
    "model = YOLO('best.pt')  # Cambia 'best.pt' por la ruta a tu modelo si está en otro lugar\n",
    "clsName = [\"casco\", \"gafas\", \"chaleco\", \"persona\"]  # Asegúrate de incluir \"persona\" si el modelo fue entrenado con esta clase\n",
    "COLORS = np.random.uniform(0, 255, size=(len(clsName), 3))\n",
    "\n",
    "# Definir la cámara fuera de las funciones para acceso global\n",
    "cap = None\n",
    "\n",
    "def update_epp_status(casco_detected, gafas_detected, chaleco_detected):\n",
    "    # Actualiza las imágenes según las detecciones\n",
    "    if casco_detected:\n",
    "        lbl_casco.config(image=img_si_casco)\n",
    "        lbl_casco.image = img_si_casco\n",
    "    else:\n",
    "        lbl_casco.config(image=img_no_casco)\n",
    "        lbl_casco.image = img_no_casco\n",
    "\n",
    "    if gafas_detected:\n",
    "        lbl_gafas.config(image=img_si_gafas)\n",
    "        lbl_gafas.image = img_si_gafas\n",
    "    else:\n",
    "        lbl_gafas.config(image=img_no_gafas)\n",
    "        lbl_gafas.image = img_no_gafas\n",
    "\n",
    "    if chaleco_detected:\n",
    "        lbl_chaleco.config(image=img_si_chaleco)\n",
    "        lbl_chaleco.image = img_si_chaleco\n",
    "    else:\n",
    "        lbl_chaleco.config(image=img_no_chaleco)\n",
    "        lbl_chaleco.image = img_no_chaleco\n",
    "\n",
    "def draw_boxes(frame, boxes):\n",
    "    casco_detected = gafas_detected = chaleco_detected = False\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "        cls = int(box.cls[0])\n",
    "        conf = box.conf[0]\n",
    "\n",
    "        # Agregar una verificación de confianza mínima\n",
    "        if conf >= 0.3:  # Ajustar el umbral de confianza según la calidad del modelo\n",
    "            # Verificar si la clase detectada está dentro de los índices válidos\n",
    "            if cls < len(clsName):\n",
    "                # Determinar si se detectó cada EPP\n",
    "                if cls == 0:  # Casco\n",
    "                    casco_detected = True\n",
    "                elif cls == 1:  # Gafas\n",
    "                    gafas_detected = True\n",
    "                elif cls == 2:  # Chaleco\n",
    "                    chaleco_detected = True\n",
    "                elif cls == 3:  # Persona\n",
    "                    # La clase \"persona\" solo se dibuja, no tiene una imagen específica\n",
    "                    pass\n",
    "\n",
    "                # Dibujar el rectángulo y el texto para todas las detecciones\n",
    "                color = COLORS[cls]\n",
    "                label = f\"{clsName[cls]} {conf:.2f}\"\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    # Actualizar la interfaz con el estado de los EPP detectados\n",
    "    update_epp_status(casco_detected, gafas_detected, chaleco_detected)\n",
    "\n",
    "def Scanning():\n",
    "    global cap  # Declarar cap como global para asegurarnos de tener acceso a la cámara\n",
    "\n",
    "    if cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "            for result in results:\n",
    "                draw_boxes(frame, result.boxes)\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(frame_rgb))\n",
    "            lblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "\n",
    "        lblVideo.after(10, Scanning)\n",
    "\n",
    "def ventana_principal():\n",
    "    global cap, lblVideo, lbl_casco, lbl_gafas, lbl_chaleco\n",
    "    global img_no_casco, img_no_gafas, img_no_chaleco\n",
    "    global img_si_casco, img_si_gafas, img_si_chaleco\n",
    "\n",
    "    pantalla = Tk()\n",
    "    pantalla.title(\"Detección de EPP\")\n",
    "    pantalla.geometry(\"1280x720\")\n",
    "\n",
    "    # Cargar la imagen de fondo\n",
    "    try:\n",
    "        imageF = Image.open(\"Canva.png\")\n",
    "        imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n",
    "        imageF = ImageTk.PhotoImage(imageF)\n",
    "        background = Label(pantalla, image=imageF)\n",
    "        background.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "        background.image = imageF\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: No se pudo cargar 'Canva.png'. Asegúrate de que la imagen esté en la misma carpeta.\")\n",
    "\n",
    "    # Configurar las rutas de las imágenes (utilizando nombres más simples)\n",
    "    base_path = os.path.abspath(\"Images\")\n",
    "\n",
    "    img_no_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_casco - copia.png\")))\n",
    "    img_no_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_gafas - copia.png\")))\n",
    "    img_no_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_chaleco - copia.png\")))\n",
    "    img_si_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_casco - copia.png\")))\n",
    "    img_si_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_gafas - copia.png\")))\n",
    "    img_si_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_chaleco - copia.png\")))\n",
    "\n",
    "    # Etiquetas para mostrar las imágenes de estado de EPP en la interfaz\n",
    "    lbl_casco = Label(pantalla, image=img_no_casco, bg='#d9d9d9')  # Fondo gris claro para que destaque\n",
    "    lbl_casco.place(x=75, y=110)\n",
    "    lbl_gafas = Label(pantalla, image=img_no_gafas, bg='#d9d9d9')\n",
    "    lbl_gafas.place(x=75, y=260)\n",
    "    lbl_chaleco = Label(pantalla, image=img_no_chaleco, bg='#d9d9d9')\n",
    "    lbl_chaleco.place(x=75, y=410)\n",
    "\n",
    "    # Configuración del video\n",
    "    lblVideo = Label(pantalla)\n",
    "    lblVideo.place(x=320, y=120)\n",
    "\n",
    "    # Configurar la cámara\n",
    "    cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # Intentar con la cámara 0 si la 1 no funciona\n",
    "    cap.set(3, 640)\n",
    "    cap.set(4, 480)\n",
    "\n",
    "    # Iniciar la detección\n",
    "    Scanning()\n",
    "    pantalla.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ventana_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@@@@@ CAMARA DETECTANDO TODO SIN INTERFAZ @@@@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando video desde la cámara...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "\n",
    "# Cargar tu modelo YOLO personalizado\n",
    "model = YOLO(\"best.pt\")  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = [\"arnes\", \"botas\", \"casco\", \"chalecos\", \"guantes\", \"lentes\", \"persona\"]  # Clases de tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Utilizar la cámara para capturar el video\n",
    "cap = cv2.VideoCapture(1)  # 0 indica la cámara por defecto del sistema\n",
    "\n",
    "# Verificamos si la cámara se ha abierto correctamente\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"No se pudo acceder a la cámara.\")\n",
    "\n",
    "# Obtener la información de la cámara (ancho, alto, FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# (Opcional) Guardar el video procesado\n",
    "output_video_path = \"ppe_camera_output.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec para el video\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "print(f\"Procesando video desde la cámara...\")\n",
    "\n",
    "# Función para dibujar las cajas y verificar si se detectan diferentes equipos de protección personal\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "        \n",
    "        # Solo dibujar la caja si la confianza es mayor o igual a 0.5\n",
    "        if box.conf[0] >= 0.5:\n",
    "            # Extraemos las coordenadas de la caja\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "\n",
    "            # Dibujamos la caja sobre el frame con un borde grueso (4 píxeles)\n",
    "            color = colors[c]\n",
    "            cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 4)\n",
    "\n",
    "            # Etiqueta con el nombre de la clase\n",
    "            label = f\"{classes[c]}\"\n",
    "\n",
    "            # Definir el tamaño del texto y la ubicación\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            \n",
    "            # Crear un fondo sólido para la etiqueta\n",
    "            cv2.rectangle(frame, (r[0], r[1] - h - 10), (r[0] + w, r[1]), color, -1)\n",
    "\n",
    "            # Colocar el texto sobre el fondo\n",
    "            cv2.putText(frame, label, (r[0], r[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "# Bucle principal para capturar frames de la cámara y procesar las detecciones\n",
    "while cap.isOpened():\n",
    "    # Leemos un frame de la cámara\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"No se pudo leer el frame o se ha perdido la conexión con la cámara.\")\n",
    "        break\n",
    "\n",
    "    # Realizamos la predicción con tu modelo entrenado YOLO\n",
    "    results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "    # Procesamos cada resultado detectado\n",
    "    for result in results:\n",
    "        draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "    # Mostramos el frame con las detecciones en una ventana de OpenCV\n",
    "    cv2.imshow('Detección de EPP - Cámara', frame)\n",
    "\n",
    "    # (Opcional) Guardamos el frame procesado en el video de salida\n",
    "    out.write(frame)\n",
    "\n",
    "    # Verificamos si se presiona la tecla 'q' para salir\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos la cámara y cerramos todas las ventanas\n",
    "cap.release()\n",
    "out.release()  # Liberamos el video de salida\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interfaz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm246\\AppData\\Local\\Temp\\ipykernel_7172\\2476614020.py:134: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "from tkinter import Tk, Label\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "\n",
    "# Definir el modelo YOLO como global\n",
    "model = YOLO('best.pt')  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = [\"arnes\", \"botas\", \"casco\", \"chalecos\", \"guantes\", \"lentes\", \"persona\"]  # Clases de tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Definir la cámara fuera de las funciones para acceso global\n",
    "cap = None\n",
    "\n",
    "# Definir las etiquetas de estado de EPP\n",
    "lbl_casco = None\n",
    "lbl_gafas = None\n",
    "lbl_chaleco = None\n",
    "\n",
    "# Cargar las imágenes de estado de EPP cuando la ventana ya esté creada\n",
    "def cargar_imagenes():\n",
    "    base_path = os.path.abspath(\"Images\")\n",
    "\n",
    "    global img_no_casco, img_no_gafas, img_no_chaleco\n",
    "    global img_si_casco, img_si_gafas, img_si_chaleco\n",
    "\n",
    "    img_no_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_casco - copia.png\")))\n",
    "    img_no_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_gafas - copia.png\")))\n",
    "    img_no_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_chaleco - copia.png\")))\n",
    "    img_si_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_casco - copia.png\")))\n",
    "    img_si_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_gafas - copia.png\")))\n",
    "    img_si_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_chaleco - copia.png\")))\n",
    "\n",
    "\n",
    "def update_epp_status(casco_detected, gafas_detected, chaleco_detected):\n",
    "    # Actualiza las imágenes según las detecciones\n",
    "    if casco_detected:\n",
    "        lbl_casco.config(image=img_si_casco)\n",
    "        lbl_casco.image = img_si_casco\n",
    "    else:\n",
    "        lbl_casco.config(image=img_no_casco)\n",
    "        lbl_casco.image = img_no_casco\n",
    "\n",
    "    if gafas_detected:\n",
    "        lbl_gafas.config(image=img_si_gafas)\n",
    "        lbl_gafas.image = img_si_gafas\n",
    "    else:\n",
    "        lbl_gafas.config(image=img_no_gafas)\n",
    "        lbl_gafas.image = img_no_gafas\n",
    "\n",
    "    if chaleco_detected:\n",
    "        lbl_chaleco.config(image=img_si_chaleco)\n",
    "        lbl_chaleco.image = img_si_chaleco\n",
    "    else:\n",
    "        lbl_chaleco.config(image=img_no_chaleco)\n",
    "        lbl_chaleco.image = img_no_chaleco\n",
    "\n",
    "\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    casco_detected = gafas_detected = chaleco_detected = False\n",
    "\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "        \n",
    "        # Solo dibujar la caja si la confianza es mayor o igual a 0.5\n",
    "        if box.conf[0] >= 0.5:\n",
    "            # Extraemos las coordenadas de la caja\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "\n",
    "            # Dibujamos la caja sobre el frame con un borde grueso (4 píxeles)\n",
    "            color = colors[c]\n",
    "            cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 4)\n",
    "\n",
    "            # Etiqueta con el nombre de la clase\n",
    "            label = f\"{labels[c]}\"\n",
    "\n",
    "            # Definir el tamaño del texto y la ubicación\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            \n",
    "            # Crear un fondo sólido para la etiqueta\n",
    "            cv2.rectangle(frame, (r[0], r[1] - h - 10), (r[0] + w, r[1]), color, -1)\n",
    "\n",
    "            # Colocar el texto sobre el fondo\n",
    "            cv2.putText(frame, label, (r[0], r[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "            # Actualizar estado de EPP detectado\n",
    "            if labels[c] == \"casco\":\n",
    "                casco_detected = True\n",
    "            elif labels[c] == \"lentes\":  # Cambié \"gafas\" por \"lentes\" para que coincida con la clase definida\n",
    "                gafas_detected = True\n",
    "            elif labels[c] == \"chalecos\":\n",
    "                chaleco_detected = True\n",
    "\n",
    "    # Actualizar la interfaz con el estado de los EPP detectados\n",
    "    update_epp_status(casco_detected, gafas_detected, chaleco_detected)\n",
    "\n",
    "\n",
    "def Scanning():\n",
    "    global cap  # Declarar cap como global para asegurarnos de tener acceso a la cámara\n",
    "\n",
    "    if cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "            for result in results:\n",
    "                draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(frame_rgb))\n",
    "            lblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "\n",
    "        lblVideo.after(10, Scanning)\n",
    "\n",
    "\n",
    "def ventana_principal():\n",
    "    global cap, lblVideo, lbl_casco, lbl_gafas, lbl_chaleco\n",
    "\n",
    "    pantalla = Tk()\n",
    "    pantalla.title(\"Detección de EPP\")\n",
    "    pantalla.geometry(\"1280x720\")\n",
    "\n",
    "    # Cargar la imagen de fondo\n",
    "    try:\n",
    "        imageF = Image.open(\"Canva.png\")\n",
    "        imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n",
    "        imageF = ImageTk.PhotoImage(imageF)\n",
    "        background = Label(pantalla, image=imageF)\n",
    "        background.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "        background.image = imageF\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: No se pudo cargar 'Canva.png'. Asegúrate de que la imagen esté en la misma carpeta.\")\n",
    "\n",
    "    # Cargar las imágenes una vez que la ventana se haya creado\n",
    "    cargar_imagenes()\n",
    "\n",
    "    # Configurar las etiquetas para mostrar las imágenes de estado de EPP en la interfaz\n",
    "    lbl_casco = Label(pantalla, image=img_no_casco, bg='#d9d9d9')  # Fondo gris claro para que destaque\n",
    "    lbl_casco.place(x=75, y=110)\n",
    "    lbl_gafas = Label(pantalla, image=img_no_gafas, bg='#d9d9d9')\n",
    "    lbl_gafas.place(x=75, y=260)\n",
    "    lbl_chaleco = Label(pantalla, image=img_no_chaleco, bg='#d9d9d9')\n",
    "    lbl_chaleco.place(x=75, y=410)\n",
    "\n",
    "    # Configuración del video\n",
    "    lblVideo = Label(pantalla)\n",
    "    lblVideo.place(x=320, y=120)\n",
    "\n",
    "    # Configurar la cámara\n",
    "    cap = cv2.VideoCapture(1)  # 0 indica la cámara por defecto del sistema\n",
    "    if not cap.isOpened():\n",
    "        sys.exit(\"No se pudo acceder a la cámara.\")\n",
    "\n",
    "    # Iniciar la detección\n",
    "    Scanning()\n",
    "    pantalla.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ventana_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### codigo funcional - mas entrenamiento para mayor presicion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm246\\AppData\\Local\\Temp\\ipykernel_9196\\3419706685.py:134: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "from tkinter import Tk, Label\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "\n",
    "# Definir el modelo YOLO como global\n",
    "model = YOLO('best-3.pt')  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = ['Gafas', 'Casco', 'Persona', 'Chaleco']  # Clases de tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Definir la cámara fuera de las funciones para acceso global\n",
    "cap = None\n",
    "\n",
    "# Definir las etiquetas de estado de EPP\n",
    "lbl_casco = None\n",
    "lbl_gafas = None\n",
    "lbl_chaleco = None\n",
    "\n",
    "# Cargar las imágenes de estado de EPP cuando la ventana ya esté creada\n",
    "def cargar_imagenes():\n",
    "    base_path = os.path.abspath(\"Images\")\n",
    "\n",
    "    global img_no_casco, img_no_gafas, img_no_chaleco\n",
    "    global img_si_casco, img_si_gafas, img_si_chaleco\n",
    "\n",
    "    img_no_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_casco - copia.png\")))\n",
    "    img_no_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_gafas - copia.png\")))\n",
    "    img_no_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_chaleco - copia.png\")))\n",
    "    img_si_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_casco - copia.png\")))\n",
    "    img_si_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_gafas - copia.png\")))\n",
    "    img_si_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_chaleco - copia.png\")))\n",
    "\n",
    "\n",
    "def update_epp_status(casco_detected, gafas_detected, chaleco_detected):\n",
    "    # Actualiza las imágenes según las detecciones\n",
    "    if casco_detected:\n",
    "        lbl_casco.config(image=img_si_casco)\n",
    "        lbl_casco.image = img_si_casco\n",
    "    else:\n",
    "        lbl_casco.config(image=img_no_casco)\n",
    "        lbl_casco.image = img_no_casco\n",
    "\n",
    "    if gafas_detected:\n",
    "        lbl_gafas.config(image=img_si_gafas)\n",
    "        lbl_gafas.image = img_si_gafas\n",
    "    else:\n",
    "        lbl_gafas.config(image=img_no_gafas)\n",
    "        lbl_gafas.image = img_no_gafas\n",
    "\n",
    "    if chaleco_detected:\n",
    "        lbl_chaleco.config(image=img_si_chaleco)\n",
    "        lbl_chaleco.image = img_si_chaleco\n",
    "    else:\n",
    "        lbl_chaleco.config(image=img_no_chaleco)\n",
    "        lbl_chaleco.image = img_no_chaleco\n",
    "\n",
    "\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    casco_detected = gafas_detected = chaleco_detected = False\n",
    "\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "        \n",
    "        # Solo dibujar la caja si la confianza es mayor o igual a 0.5\n",
    "        if box.conf[0] >= 0.5:\n",
    "            # Extraemos las coordenadas de la caja\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "\n",
    "            # Dibujamos la caja sobre el frame con un borde grueso (4 píxeles)\n",
    "            color = colors[c]\n",
    "            cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 4)\n",
    "\n",
    "            # Etiqueta con el nombre de la clase\n",
    "            label = f\"{labels[c]}\"\n",
    "\n",
    "            # Definir el tamaño del texto y la ubicación\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            \n",
    "            # Crear un fondo sólido para la etiqueta\n",
    "            cv2.rectangle(frame, (r[0], r[1] - h - 10), (r[0] + w, r[1]), color, -1)\n",
    "\n",
    "            # Colocar el texto sobre el fondo\n",
    "            cv2.putText(frame, label, (r[0], r[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "            # Actualizar estado de EPP detectado\n",
    "            if labels[c] == \"Casco\":\n",
    "                casco_detected = True\n",
    "            elif labels[c] == \"Gafas\":\n",
    "                gafas_detected = True\n",
    "            elif labels[c] == \"Chaleco\":\n",
    "                chaleco_detected = True\n",
    "\n",
    "    # Actualizar la interfaz con el estado de los EPP detectados\n",
    "    update_epp_status(casco_detected, gafas_detected, chaleco_detected)\n",
    "\n",
    "\n",
    "def Scanning():\n",
    "    global cap  # Declarar cap como global para asegurarnos de tener acceso a la cámara\n",
    "\n",
    "    if cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "            for result in results:\n",
    "                draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(frame_rgb))\n",
    "            lblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "\n",
    "        lblVideo.after(10, Scanning)\n",
    "\n",
    "\n",
    "def ventana_principal():\n",
    "    global cap, lblVideo, lbl_casco, lbl_gafas, lbl_chaleco\n",
    "\n",
    "    pantalla = Tk()\n",
    "    pantalla.title(\"Detección de EPP\")\n",
    "    pantalla.geometry(\"1280x720\")\n",
    "\n",
    "    # Cargar la imagen de fondo\n",
    "    try:\n",
    "        imageF = Image.open(\"Canva.png\")\n",
    "        imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n",
    "        imageF = ImageTk.PhotoImage(imageF)\n",
    "        background = Label(pantalla, image=imageF)\n",
    "        background.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "        background.image = imageF\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: No se pudo cargar 'Canva.png'. Asegúrate de que la imagen esté en la misma carpeta.\")\n",
    "\n",
    "    # Cargar las imágenes una vez que la ventana se haya creado\n",
    "    cargar_imagenes()\n",
    "\n",
    "    # Configurar las etiquetas para mostrar las imágenes de estado de EPP en la interfaz\n",
    "    lbl_casco = Label(pantalla, image=img_no_casco, bg='#d9d9d9')  # Fondo gris claro para que destaque\n",
    "    lbl_casco.place(x=75, y=110)\n",
    "    lbl_gafas = Label(pantalla, image=img_no_gafas, bg='#d9d9d9')\n",
    "    lbl_gafas.place(x=75, y=260)\n",
    "    lbl_chaleco = Label(pantalla, image=img_no_chaleco, bg='#d9d9d9')\n",
    "    lbl_chaleco.place(x=75, y=410)\n",
    "\n",
    "    # Configuración del video\n",
    "    lblVideo = Label(pantalla)\n",
    "    lblVideo.place(x=320, y=120)\n",
    "\n",
    "    # Configurar la cámara\n",
    "    cap = cv2.VideoCapture(1)  # 0 indica la cámara por defecto del sistema\n",
    "    if not cap.isOpened():\n",
    "        sys.exit(\"No se pudo acceder a la cámara.\")\n",
    "\n",
    "    # Iniciar la detección\n",
    "    Scanning()\n",
    "    pantalla.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ventana_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm246\\AppData\\Local\\Temp\\ipykernel_12368\\3516722037.py:148: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "from tkinter import Tk, Label\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "\n",
    "# Definir el modelo YOLO como global\n",
    "model = YOLO('best-3.pt')  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = ['Gafas', 'Casco', 'Persona', 'Chaleco']  # Clases de tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Definir la cámara fuera de las funciones para acceso global\n",
    "cap = None\n",
    "\n",
    "# Definir las etiquetas de estado de EPP\n",
    "lbl_casco = None\n",
    "lbl_gafas = None\n",
    "lbl_chaleco = None\n",
    "\n",
    "# Crear directorio para almacenar los recortes de detecciones\n",
    "output_dir = \"Resultados\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Cargar las imágenes de estado de EPP cuando la ventana ya esté creada\n",
    "def cargar_imagenes():\n",
    "    base_path = os.path.abspath(\"Images\")\n",
    "\n",
    "    global img_no_casco, img_no_gafas, img_no_chaleco\n",
    "    global img_si_casco, img_si_gafas, img_si_chaleco\n",
    "\n",
    "    img_no_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_casco - copia.png\")))\n",
    "    img_no_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_gafas - copia.png\")))\n",
    "    img_no_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_chaleco - copia.png\")))\n",
    "    img_si_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_casco - copia.png\")))\n",
    "    img_si_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_gafas - copia.png\")))\n",
    "    img_si_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_chaleco - copia.png\")))\n",
    "\n",
    "\n",
    "def update_epp_status(casco_detected, gafas_detected, chaleco_detected):\n",
    "    # Actualiza las imágenes según las detecciones\n",
    "    if casco_detected:\n",
    "        lbl_casco.config(image=img_si_casco)\n",
    "        lbl_casco.image = img_si_casco\n",
    "    else:\n",
    "        lbl_casco.config(image=img_no_casco)\n",
    "        lbl_casco.image = img_no_casco\n",
    "\n",
    "    if gafas_detected:\n",
    "        lbl_gafas.config(image=img_si_gafas)\n",
    "        lbl_gafas.image = img_si_gafas\n",
    "    else:\n",
    "        lbl_gafas.config(image=img_no_gafas)\n",
    "        lbl_gafas.image = img_no_gafas\n",
    "\n",
    "    if chaleco_detected:\n",
    "        lbl_chaleco.config(image=img_si_chaleco)\n",
    "        lbl_chaleco.image = img_si_chaleco\n",
    "    else:\n",
    "        lbl_chaleco.config(image=img_no_chaleco)\n",
    "        lbl_chaleco.image = img_no_chaleco\n",
    "\n",
    "\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    casco_detected = gafas_detected = chaleco_detected = False\n",
    "    persona_detected = False\n",
    "\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "        \n",
    "        # Solo dibujar la caja si la confianza es mayor o igual a 0.5\n",
    "        if box.conf[0] >= 0.5:\n",
    "            # Extraemos las coordenadas de la caja\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "\n",
    "            # Dibujamos la caja sobre el frame con un borde grueso (4 píxeles)\n",
    "            color = colors[c]\n",
    "            cv2.rectangle(frame, (r[0], r[1]), (r[2], r[3]), color, 4)\n",
    "\n",
    "            # Etiqueta con el nombre de la clase\n",
    "            label = f\"{labels[c]}\"\n",
    "\n",
    "            # Definir el tamaño del texto y la ubicación\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            \n",
    "            # Crear un fondo sólido para la etiqueta\n",
    "            cv2.rectangle(frame, (r[0], r[1] - h - 10), (r[0] + w, r[1]), color, -1)\n",
    "\n",
    "            # Colocar el texto sobre el fondo\n",
    "            cv2.putText(frame, label, (r[0], r[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "            # Si se detecta una persona, establecer el flag para captura de pantalla\n",
    "            if labels[c] == \"Persona\":\n",
    "                persona_detected = True\n",
    "\n",
    "            # Actualizar estado de EPP detectado\n",
    "            if labels[c] == \"Casco\":\n",
    "                casco_detected = True\n",
    "            elif labels[c] == \"Gafas\":\n",
    "                gafas_detected = True\n",
    "            elif labels[c] == \"Chaleco\":\n",
    "                chaleco_detected = True\n",
    "\n",
    "    # Si se detecta una persona, guardar captura de pantalla\n",
    "    if persona_detected:\n",
    "        filename = os.path.join(output_dir, f\"captura_persona_{len(os.listdir(output_dir))}.png\")\n",
    "        cv2.imwrite(filename, frame)\n",
    "\n",
    "    # Actualizar la interfaz con el estado de los EPP detectados\n",
    "    update_epp_status(casco_detected, gafas_detected, chaleco_detected)\n",
    "\n",
    "\n",
    "def Scanning():\n",
    "    global cap  # Declarar cap como global para asegurarnos de tener acceso a la cámara\n",
    "\n",
    "    if cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "            for result in results:\n",
    "                draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(frame_rgb))\n",
    "            lblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "\n",
    "        lblVideo.after(10, Scanning)\n",
    "\n",
    "\n",
    "def ventana_principal():\n",
    "    global cap, lblVideo, lbl_casco, lbl_gafas, lbl_chaleco\n",
    "\n",
    "    pantalla = Tk()\n",
    "    pantalla.title(\"Detección de EPP\")\n",
    "    pantalla.geometry(\"1280x720\")\n",
    "\n",
    "    # Cargar la imagen de fondo\n",
    "    try:\n",
    "        imageF = Image.open(\"Canva.png\")\n",
    "        imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n",
    "        imageF = ImageTk.PhotoImage(imageF)\n",
    "        background = Label(pantalla, image=imageF)\n",
    "        background.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "        background.image = imageF\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: No se pudo cargar 'Canva.png'. Asegúrate de que la imagen esté en la misma carpeta.\")\n",
    "\n",
    "    # Cargar las imágenes una vez que la ventana se haya creado\n",
    "    cargar_imagenes()\n",
    "\n",
    "    # Configurar las etiquetas para mostrar las imágenes de estado de EPP en la interfaz\n",
    "    lbl_casco = Label(pantalla, image=img_no_casco, bg='#d9d9d9')  # Fondo gris claro para que destaque\n",
    "    lbl_casco.place(x=75, y=110)\n",
    "    lbl_gafas = Label(pantalla, image=img_no_gafas, bg='#d9d9d9')\n",
    "    lbl_gafas.place(x=75, y=260)\n",
    "    lbl_chaleco = Label(pantalla, image=img_no_chaleco, bg='#d9d9d9')\n",
    "    lbl_chaleco.place(x=75, y=410)\n",
    "\n",
    "    # Configuración del video\n",
    "    lblVideo = Label(pantalla)\n",
    "    lblVideo.place(x=320, y=120)\n",
    "\n",
    "    # Configurar la cámara\n",
    "    cap = cv2.VideoCapture(1)  # 0 indica la cámara por defecto del sistema\n",
    "    if not cap.isOpened():\n",
    "        sys.exit(\"No se pudo acceder a la cámara.\")\n",
    "\n",
    "    # Iniciar la detección\n",
    "    Scanning()\n",
    "    pantalla.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ventana_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DETECCION SOLO EN VIDEO y CAMARA CON CROPS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "from tkinter import Tk, Label\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "\n",
    "# Definir el modelo YOLO como global\n",
    "model = YOLO('best-3.pt')  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = ['Gafas', 'Casco', 'Persona', 'Chaleco']  # Clases de tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Definir el video fuera de las funciones para acceso global\n",
    "video_path = \"video.mp4\"  # Cambia a la ruta del video que deseas procesar\n",
    "cap_video = None\n",
    "\n",
    "# Crear directorio para almacenar las capturas\n",
    "output_dir = \"Resultados\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Crear directorios para cumplir y no cumplir\n",
    "output_dir_compliance = os.path.join(output_dir, \"Cumple\")\n",
    "output_dir_non_compliance = os.path.join(output_dir, \"NoCumple\")\n",
    "os.makedirs(output_dir_compliance, exist_ok=True)\n",
    "os.makedirs(output_dir_non_compliance, exist_ok=True)\n",
    "\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    # Variables para verificar cumplimiento de EPP\n",
    "    detected_person = None\n",
    "    detected_items = set()\n",
    "    crop_margin = 50  # Aumentar el margen adicional para el cuadro\n",
    "    crop_margin_large = 70  # Margen mayor para el crop específico\n",
    "\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "        \n",
    "        # Solo dibujar la caja si la confianza es mayor o igual a 0.5\n",
    "        if box.conf[0] >= 0.5:\n",
    "            # Extraemos las coordenadas de la caja y agregamos margen para el cuadro de detección\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "            x1 = max(0, r[0] - crop_margin)\n",
    "            y1 = max(0, r[1] - crop_margin)\n",
    "            x2 = min(frame.shape[1], r[2] + crop_margin)\n",
    "            y2 = min(frame.shape[0], r[3] + crop_margin)\n",
    "\n",
    "            # Dibujamos una región transparente alrededor del objeto detectado\n",
    "            overlay = frame.copy()\n",
    "            color = colors[c]\n",
    "            alpha = 0.3  # Nivel de transparencia\n",
    "            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)\n",
    "            # Aplicar el overlay con transparencia\n",
    "            cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "\n",
    "            # Dibujamos la caja de contorno sobre el frame\n",
    "            thickness = 2\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "            # Etiqueta con el nombre de la clase\n",
    "            label = f\"{labels[c]}\"\n",
    "\n",
    "            # Definir el tamaño del texto y la ubicación\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "            \n",
    "            # Crear un fondo sólido para la etiqueta\n",
    "            cv2.rectangle(frame, (x1, y1 - h - 10), (x1 + w, y1), color, -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            # Colocar el texto sobre el fondo\n",
    "            cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "            # Realizar un crop de la persona detectada con mayor margen para obtener una imagen más amplia\n",
    "            if labels[c] == \"Persona\":\n",
    "                detected_person = frame[max(0, r[1] - crop_margin_large):min(frame.shape[0], r[3] + crop_margin_large),\n",
    "                                        max(0, r[0] - crop_margin_large):min(frame.shape[1], r[2] + crop_margin_large)]\n",
    "            elif labels[c] in {\"Gafas\", \"Casco\", \"Chaleco\"}:\n",
    "                detected_items.add(labels[c])\n",
    "\n",
    "    # Guardar el recorte de la persona si fue detectada\n",
    "    if detected_person is not None and detected_person.size > 0:  # Asegurarse de que el recorte no esté vacío\n",
    "        # Verificar si la persona cumple con todos los EPP\n",
    "        required_items = {\"Gafas\", \"Casco\", \"Chaleco\"}\n",
    "        cumple = required_items.issubset(detected_items)\n",
    "        if cumple:\n",
    "            filename = os.path.join(output_dir_compliance, f\"cumple_persona_{len(os.listdir(output_dir_compliance))}.png\")\n",
    "        else:\n",
    "            filename = os.path.join(output_dir_non_compliance, f\"nocumple_persona_{len(os.listdir(output_dir_non_compliance))}.png\")\n",
    "        # Ajustar el tamaño del crop para ser aproximadamente 550x735 píxeles\n",
    "        detected_person_resized = cv2.resize(detected_person, (550, 735))\n",
    "        cv2.imwrite(filename, detected_person_resized)\n",
    "\n",
    "\n",
    "def Scanning():\n",
    "    global cap_video  # Declarar cap_video como global para asegurarnos de tener acceso al video\n",
    "\n",
    "    if cap_video.isOpened():\n",
    "        ret_video, frame_video = cap_video.read()\n",
    "\n",
    "        if ret_video:\n",
    "            # Procesar el frame del video original (sin detecciones)\n",
    "            frame_video_rgb = cv2.cvtColor(frame_video, cv2.COLOR_BGR2RGB)\n",
    "            img_video_original = ImageTk.PhotoImage(image=Image.fromarray(frame_video_rgb))\n",
    "            lblVideoOriginal.configure(image=img_video_original)\n",
    "            lblVideoOriginal.image = img_video_original\n",
    "\n",
    "            # Procesar el frame del video con detecciones\n",
    "            results_video = model.predict(frame_video, stream=True, verbose=False)\n",
    "\n",
    "            for result in results_video:\n",
    "                draw_boxes(frame_video, result.boxes, classes, COLORS)\n",
    "\n",
    "            frame_video_rgb_detected = cv2.cvtColor(frame_video, cv2.COLOR_BGR2RGB)\n",
    "            img_video_detected = ImageTk.PhotoImage(image=Image.fromarray(frame_video_rgb_detected))\n",
    "            lblVideoDetected.configure(image=img_video_detected)\n",
    "            lblVideoDetected.image = img_video_detected\n",
    "\n",
    "        lblVideoOriginal.after(10, Scanning)\n",
    "\n",
    "\n",
    "def ventana_principal():\n",
    "    global cap_video, lblVideoOriginal, lblVideoDetected\n",
    "\n",
    "    pantalla = Tk()\n",
    "    pantalla.title(\"Detección de EPP\")\n",
    "    pantalla.geometry(\"1280x720\")\n",
    "\n",
    "    # Configuración del video (video original y video con detección)\n",
    "    lblVideoOriginal = Label(pantalla)\n",
    "    lblVideoOriginal.place(x=10, y=10)\n",
    "    lblVideoDetected = Label(pantalla)\n",
    "    lblVideoDetected.place(x=650, y=10)\n",
    "\n",
    "    # Configurar el video\n",
    "    cap_video = cv2.VideoCapture(video_path)\n",
    "    if not cap_video.isOpened():\n",
    "        sys.exit(f\"No se pudo abrir el archivo de video: {video_path}\")\n",
    "\n",
    "    # Iniciar la detección\n",
    "    Scanning()\n",
    "    pantalla.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ventana_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "No se pudo abrir el archivo de video: video Construction Site.mp4",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m No se pudo abrir el archivo de video: video Construction Site.mp4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "from tkinter import Tk, Label\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "\n",
    "# Definir el modelo YOLO como global\n",
    "model = YOLO('best-3.pt')  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = ['Gafas', 'Casco', 'Persona', 'Chaleco']  # Clases de tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Definir el video fuera de las funciones para acceso global\n",
    "video_path = \"video Construction Site.mp4\"  # Cambia a la ruta del video que deseas procesar\n",
    "cap_video = None\n",
    "\n",
    "# Crear directorio para almacenar las capturas\n",
    "output_dir = \"Resultados\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Crear directorios para cumplir y no cumplir\n",
    "output_dir_compliance = os.path.join(output_dir, \"Cumple\")\n",
    "output_dir_non_compliance = os.path.join(output_dir, \"NoCumple\")\n",
    "os.makedirs(output_dir_compliance, exist_ok=True)\n",
    "os.makedirs(output_dir_non_compliance, exist_ok=True)\n",
    "\n",
    "def draw_dashed_rectangle(frame, pt1, pt2, color, thickness=2, dash_length=10):\n",
    "    # Dibuja un rectángulo con líneas discontinuas usando dash_length en cada lado\n",
    "    x1, y1 = pt1\n",
    "    x2, y2 = pt2\n",
    "    \n",
    "    # Lado superior\n",
    "    for i in range(x1, x2, dash_length * 2):\n",
    "        start = (i, y1)\n",
    "        end = (min(i + dash_length, x2), y1)\n",
    "        cv2.line(frame, start, end, color, thickness)\n",
    "    \n",
    "    # Lado inferior\n",
    "    for i in range(x1, x2, dash_length * 2):\n",
    "        start = (i, y2)\n",
    "        end = (min(i + dash_length, x2), y2)\n",
    "        cv2.line(frame, start, end, color, thickness)\n",
    "    \n",
    "    # Lado izquierdo\n",
    "    for i in range(y1, y2, dash_length * 2):\n",
    "        start = (x1, i)\n",
    "        end = (x1, min(i + dash_length, y2))\n",
    "        cv2.line(frame, start, end, color, thickness)\n",
    "    \n",
    "    # Lado derecho\n",
    "    for i in range(y1, y2, dash_length * 2):\n",
    "        start = (x2, i)\n",
    "        end = (x2, min(i + dash_length, y2))\n",
    "        cv2.line(frame, start, end, color, thickness)\n",
    "\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    # Variables para verificar cumplimiento de EPP\n",
    "    detected_person = None\n",
    "    detected_items = set()\n",
    "    crop_margin = 50  # Aumentar el margen adicional para el cuadro\n",
    "    crop_margin_large = 70  # Margen mayor para el crop específico\n",
    "\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "        \n",
    "        # Solo dibujar la caja si la confianza es mayor o igual a 0.5\n",
    "        if box.conf[0] >= 0.5:\n",
    "            # Extraemos las coordenadas de la caja y agregamos margen para el cuadro de detección\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "            x1 = max(0, r[0] - crop_margin)\n",
    "            y1 = max(0, r[1] - crop_margin)\n",
    "            x2 = min(frame.shape[1], r[2] + crop_margin)\n",
    "            y2 = min(frame.shape[0], r[3] + crop_margin)\n",
    "\n",
    "            # Dibujamos un cuadro discontínuo alrededor del objeto detectado\n",
    "            color = colors[c]\n",
    "            draw_dashed_rectangle(frame, (x1, y1), (x2, y2), color, thickness=2, dash_length=10)\n",
    "\n",
    "            # Etiqueta con el nombre de la clase\n",
    "            label = f\"{labels[c]}\"\n",
    "\n",
    "            # Definir el tamaño del texto y la ubicación\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "            \n",
    "            # Crear un fondo sólido para la etiqueta\n",
    "            cv2.rectangle(frame, (x1, y1 - h - 10), (x1 + w, y1), color, -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            # Colocar el texto sobre el fondo\n",
    "            cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "            # Realizar un crop de la persona detectada con mayor margen para obtener una imagen más amplia\n",
    "            if labels[c] == \"Persona\":\n",
    "                detected_person = frame[max(0, r[1] - crop_margin_large):min(frame.shape[0], r[3] + crop_margin_large),\n",
    "                                        max(0, r[0] - crop_margin_large):min(frame.shape[1], r[2] + crop_margin_large)]\n",
    "            elif labels[c] in {\"Gafas\", \"Casco\", \"Chaleco\"}:\n",
    "                detected_items.add(labels[c])\n",
    "\n",
    "    # Guardar el recorte de la persona si fue detectada\n",
    "    if detected_person is not None and detected_person.size > 0:  # Asegurarse de que el recorte no esté vacío\n",
    "        # Verificar si la persona cumple con todos los EPP\n",
    "        required_items = {\"Gafas\", \"Casco\", \"Chaleco\"}\n",
    "        cumple = required_items.issubset(detected_items)\n",
    "        if cumple:\n",
    "            filename = os.path.join(output_dir_compliance, f\"cumple_persona_{len(os.listdir(output_dir_compliance))}.png\")\n",
    "        else:\n",
    "            filename = os.path.join(output_dir_non_compliance, f\"nocumple_persona_{len(os.listdir(output_dir_non_compliance))}.png\")\n",
    "        # Ajustar el tamaño del crop para ser aproximadamente 550x735 píxeles\n",
    "        detected_person_resized = cv2.resize(detected_person, (550, 735))\n",
    "        cv2.imwrite(filename, detected_person_resized)\n",
    "\n",
    "def Scanning():\n",
    "    global cap_video  # Declarar cap_video como global para asegurarnos de tener acceso al video\n",
    "\n",
    "    if cap_video.isOpened():\n",
    "        ret_video, frame_video = cap_video.read()\n",
    "\n",
    "        if ret_video:\n",
    "            # Procesar el frame del video con detecciones\n",
    "            results_video = model.predict(frame_video, stream=True, verbose=False)\n",
    "\n",
    "            for result in results_video:\n",
    "                draw_boxes(frame_video, result.boxes, classes, COLORS)\n",
    "\n",
    "            # Convertir el frame a RGB y mostrar en la interfaz\n",
    "            frame_video_rgb_detected = cv2.cvtColor(frame_video, cv2.COLOR_BGR2RGB)\n",
    "            img_video_detected = ImageTk.PhotoImage(image=Image.fromarray(frame_video_rgb_detected))\n",
    "            lblVideoDetected.configure(image=img_video_detected)\n",
    "            lblVideoDetected.image = img_video_detected\n",
    "\n",
    "        lblVideoDetected.after(10, Scanning)\n",
    "\n",
    "def ventana_principal():\n",
    "    global cap_video, lblVideoDetected\n",
    "\n",
    "    # Configurar el video\n",
    "    cap_video = cv2.VideoCapture(video_path)\n",
    "    if not cap_video.isOpened():\n",
    "        sys.exit(f\"No se pudo abrir el archivo de video: {video_path}\")\n",
    "\n",
    "    # Obtener el tamaño del video\n",
    "    video_width = int(cap_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_height = int(cap_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Crear la ventana principal con el tamaño del video y deshabilitar el redimensionado\n",
    "    pantalla = Tk()\n",
    "    pantalla.title(\"Detección de EPP\")\n",
    "    pantalla.geometry(f\"{video_width}x{video_height}\")\n",
    "    pantalla.resizable(False, False)  # Deshabilitar el redimensionado\n",
    "\n",
    "    # Configuración del video (video con detección)\n",
    "    lblVideoDetected = Label(pantalla)\n",
    "    lblVideoDetected.place(x=0, y=0)\n",
    "\n",
    "    # Iniciar la detección\n",
    "    Scanning()\n",
    "    pantalla.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ventana_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm246\\AppData\\Local\\Temp\\ipykernel_6612\\3536125868.py:171: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Importa la clase YOLO correctamente\n",
    "import numpy as np\n",
    "from tkinter import Tk, Label\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "\n",
    "# Definir el modelo YOLO como global\n",
    "model = YOLO('best-3.pt')  # Asegúrate de usar la clase/método correcto para YOLO\n",
    "\n",
    "# Definir las clases de tu modelo entrenado\n",
    "classes = ['Gafas', 'Casco', 'Persona', 'Chaleco']  # Clases de tu modelo\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))  # Colores aleatorios para cada clase\n",
    "\n",
    "# Definir la cámara fuera de las funciones para acceso global\n",
    "cap = None\n",
    "\n",
    "# Definir las etiquetas de estado de EPP\n",
    "lbl_casco = None\n",
    "lbl_gafas = None\n",
    "lbl_chaleco = None\n",
    "\n",
    "# Crear directorio para almacenar los recortes de detecciones\n",
    "output_dir = \"Resultados_Camara\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Crear directorios para cumplir y no cumplir\n",
    "output_dir_compliance = os.path.join(output_dir, \"Cumple\")\n",
    "output_dir_non_compliance = os.path.join(output_dir, \"NoCumple\")\n",
    "os.makedirs(output_dir_compliance, exist_ok=True)\n",
    "os.makedirs(output_dir_non_compliance, exist_ok=True)\n",
    "\n",
    "# Cargar las imágenes de estado de EPP cuando la ventana ya esté creada\n",
    "def cargar_imagenes():\n",
    "    base_path = os.path.abspath(\"Images\")\n",
    "\n",
    "    global img_no_casco, img_no_gafas, img_no_chaleco\n",
    "    global img_si_casco, img_si_gafas, img_si_chaleco\n",
    "\n",
    "    img_no_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_casco - copia.png\")))\n",
    "    img_no_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_gafas - copia.png\")))\n",
    "    img_no_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"no_chaleco - copia.png\")))\n",
    "    img_si_casco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_casco - copia.png\")))\n",
    "    img_si_gafas = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_gafas - copia.png\")))\n",
    "    img_si_chaleco = ImageTk.PhotoImage(Image.open(os.path.join(base_path, \"si_chaleco - copia.png\")))\n",
    "\n",
    "\n",
    "def update_epp_status(casco_detected, gafas_detected, chaleco_detected):\n",
    "    # Actualiza las imágenes según las detecciones\n",
    "    if casco_detected:\n",
    "        lbl_casco.config(image=img_si_casco)\n",
    "        lbl_casco.image = img_si_casco\n",
    "    else:\n",
    "        lbl_casco.config(image=img_no_casco)\n",
    "        lbl_casco.image = img_no_casco\n",
    "\n",
    "    if gafas_detected:\n",
    "        lbl_gafas.config(image=img_si_gafas)\n",
    "        lbl_gafas.image = img_si_gafas\n",
    "    else:\n",
    "        lbl_gafas.config(image=img_no_gafas)\n",
    "        lbl_gafas.image = img_no_gafas\n",
    "\n",
    "    if chaleco_detected:\n",
    "        lbl_chaleco.config(image=img_si_chaleco)\n",
    "        lbl_chaleco.image = img_si_chaleco\n",
    "    else:\n",
    "        lbl_chaleco.config(image=img_no_chaleco)\n",
    "        lbl_chaleco.image = img_no_chaleco\n",
    "\n",
    "\n",
    "def draw_boxes(frame, boxes, labels, colors):\n",
    "    casco_detected = gafas_detected = chaleco_detected = False\n",
    "    detected_person = None\n",
    "    detected_items = set()\n",
    "    casco_detected = gafas_detected = chaleco_detected = False\n",
    "    detected_person = None\n",
    "    detected_items = set()\n",
    "    crop_margin_large = 70  # Margen mayor para el crop específico\n",
    "\n",
    "    for box in boxes:\n",
    "        # Obtenemos la clase del objeto detectado\n",
    "        c = int(box.cls)\n",
    "        \n",
    "        # Solo dibujar la caja si la confianza es mayor o igual a 0.5\n",
    "        if box.conf[0] >= 0.5:\n",
    "            # Extraemos las coordenadas de la caja\n",
    "            r = box.xyxy[0].cpu().numpy()  # Convertimos a numpy\n",
    "            r = [int(coord) for coord in r]  # Convertimos a enteros\n",
    "            x1 = max(0, r[0])\n",
    "            y1 = max(0, r[1])\n",
    "            x2 = min(frame.shape[1], r[2])\n",
    "            y2 = min(frame.shape[0], r[3])\n",
    "\n",
    "            # Dibujamos la caja sobre el frame con un borde grueso (4 píxeles)\n",
    "            color = colors[c]\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n",
    "\n",
    "            # Etiqueta con el nombre de la clase\n",
    "            label = f\"{labels[c]}\"\n",
    "\n",
    "            # Definir el tamaño del texto y la ubicación\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            \n",
    "            # Crear un fondo sólido para la etiqueta\n",
    "            cv2.rectangle(frame, (x1, y1 - h - 10), (x1 + w, y1), color, -1)\n",
    "\n",
    "            # Colocar el texto sobre el fondo\n",
    "            cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "            # Realizar un crop de la persona detectada con mayor margen para obtener una imagen más amplia\n",
    "            if labels[c] == \"Persona\":\n",
    "                detected_person = frame[max(0, y1 - crop_margin_large):min(frame.shape[0], y2 + crop_margin_large),\n",
    "                                        max(0, x1 - crop_margin_large):min(frame.shape[1], x2 + crop_margin_large)]\n",
    "            elif labels[c] == \"Gafas\":\n",
    "                gafas_detected = True\n",
    "            elif labels[c] == \"Casco\":\n",
    "                casco_detected = True\n",
    "            elif labels[c] == \"Chaleco\":\n",
    "                chaleco_detected = True\n",
    "                detected_items.add(labels[c])\n",
    "\n",
    "    # Guardar el recorte de la persona si fue detectada\n",
    "    if detected_person is not None and detected_person.size > 0:  # Asegurarse de que el recorte no esté vacío\n",
    "        # Verificar si la persona cumple con todos los EPP\n",
    "        required_items = {\"Gafas\", \"Casco\", \"Chaleco\"}\n",
    "        cumple = required_items.issubset(detected_items)\n",
    "        if cumple:\n",
    "            filename = os.path.join(output_dir_compliance, f\"cumple_persona_{len([f for f in os.listdir(output_dir_compliance) if f.endswith('.png')])}.png\")\n",
    "        else:\n",
    "            filename = os.path.join(output_dir_non_compliance, f\"nocumple_persona_{len([f for f in os.listdir(output_dir_non_compliance) if f.endswith('.png')])}.png\")\n",
    "        # Ajustar el tamaño del crop para ser aproximadamente 550x735 píxeles\n",
    "        detected_person_resized = cv2.resize(detected_person, (550, 735))\n",
    "        cv2.imwrite(filename, detected_person_resized)\n",
    "\n",
    "    # Actualizar la interfaz con el estado de los EPP detectados\n",
    "    update_epp_status(casco_detected, gafas_detected, chaleco_detected)\n",
    "\n",
    "\n",
    "def Scanning():\n",
    "    global cap  # Declarar cap como global para asegurarnos de tener acceso a la cámara\n",
    "\n",
    "    if cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            results = model.predict(frame, stream=True, verbose=False)\n",
    "\n",
    "            for result in results:\n",
    "                draw_boxes(frame, result.boxes, classes, COLORS)\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = ImageTk.PhotoImage(image=Image.fromarray(frame_rgb))\n",
    "            lblVideo.configure(image=img)\n",
    "            lblVideo.image = img\n",
    "\n",
    "        lblVideo.after(10, Scanning)\n",
    "\n",
    "\n",
    "def ventana_principal():\n",
    "    global cap, lblVideo, lbl_casco, lbl_gafas, lbl_chaleco\n",
    "\n",
    "    pantalla = Tk()\n",
    "    pantalla.title(\"Detección de EPP\")\n",
    "    pantalla.geometry(\"1280x720\")\n",
    "\n",
    "    # Cargar la imagen de fondo\n",
    "    try:\n",
    "        imageF = Image.open(\"canva.jpg\")\n",
    "        imageF = imageF.resize((1280, 720), Image.ANTIALIAS)\n",
    "        imageF = ImageTk.PhotoImage(imageF)\n",
    "        background = Label(pantalla, image=imageF)\n",
    "        background.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "        background.image = imageF\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: No se pudo cargar 'Canva.png'. Asegúrate de que la imagen esté en la misma carpeta.\")\n",
    "\n",
    "    # Cargar las imágenes una vez que la ventana se haya creado\n",
    "    cargar_imagenes()\n",
    "\n",
    "    # Configurar las etiquetas para mostrar las imágenes de estado de EPP en la interfaz\n",
    "    lbl_casco = Label(pantalla, image=img_no_casco, bg='#d9d9d9')  # Fondo gris claro para que destaque\n",
    "    lbl_casco.place(x=75, y=110)\n",
    "    lbl_gafas = Label(pantalla, image=img_no_gafas, bg='#d9d9d9')\n",
    "    lbl_gafas.place(x=75, y=260)\n",
    "    lbl_chaleco = Label(pantalla, image=img_no_chaleco, bg='#d9d9d9')\n",
    "    lbl_chaleco.place(x=75, y=410)\n",
    "\n",
    "    # Configuración del video\n",
    "    lblVideo = Label(pantalla)\n",
    "    lblVideo.place(x=320, y=120)\n",
    "\n",
    "    # Configurar la cámara\n",
    "    cap = cv2.VideoCapture(1)  # 0 indica la cámara por defecto del sistema\n",
    "    if not cap.isOpened():\n",
    "        sys.exit(\"No se pudo acceder a la cámara.\")\n",
    "\n",
    "    # Iniciar la detección\n",
    "    Scanning()\n",
    "    pantalla.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ventana_principal()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
